# Support Copilot+ PC Hardware and Enhanced AI Providers - Research Summary

## Overview
Research completed on expanding ImageDescriber's AI provider support to include Copilot+ PC hardware acceleration and enhanced model options for detailed image descriptions.

## Current State
ImageDescriber currently supports:
- **Ollama** (local models)
- **OpenAI** (cloud-based GPT-4 Vision)  
- **HuggingFace** (transformers-based models)

## 1. Copilot+ PC Support

### Hardware Requirements
- **NPU (Neural Processing Unit)**: 40+ TOPS (Trillion Operations Per Second)
- **Supported Processors**:
  - AMD Ryzen AI 300 Series
  - Intel Core Ultra 200V Series
  - Snapdragon X Series
- **OS Requirements**: Windows 11 with May 2024 updates
- **SDK**: Windows App SDK 1.7.1+ for AI APIs

### Available Windows AI APIs
1. **Image Description API**
   - Brief - Short caption
   - Detailed - Longer, comprehensive description  
   - Accessible - Specifically designed for screen readers
   - Diagram - For charts and diagrams
2. **Phi Silica** - Small Language Model for text generation
3. **Image Super Resolution** - Upscaling and sharpening (up to 8x)

### Implementation Requirements
- Package identity at runtime for API access
- WinRT/Windows API integration
- Hardware capability detection
- Graceful fallback to other providers

### Benefits
- **Local Processing**: No internet required, faster responses
- **Privacy**: Data stays on device
- **Cost**: No API fees after development
- **Performance**: Hardware-accelerated AI inference
- **Integration**: Native Windows experience

### Challenges
- **Limited Availability**: Only works on Copilot+ PCs (small market share)
- **Development Complexity**: Requires WinRT/Windows API knowledge
- **Testing Requirements**: Need actual Copilot+ PC hardware
- **Platform Lock-in**: Windows-only solution

## 2. ONNX Runtime Support

### Benefits
- **Cross-platform compatibility**: Windows, Linux, macOS, mobile, web
- **Hardware acceleration**: CPU, GPU, NPU support
- **Performance**: Optimized inference with quantization
- **Ecosystem**: Large selection of pre-converted models
- **Future-proof**: Works with latest AI hardware

### Dependencies to Add
```
onnxruntime>=1.16.0          # CPU inference
onnxruntime-gpu>=1.16.0      # GPU acceleration (optional)
onnx>=1.15.0                 # Model loading utilities
```

### Available Models
1. **BLIP Models** (Salesforce)
   - blip-image-captioning-base (~500MB)
   - blip-image-captioning-large (~1.2GB)
2. **ViT-GPT2** (Microsoft/OpenAI)
   - vit-gpt2-image-captioning (~300MB)
3. **GIT Models** (Microsoft)
   - git-base-coco (~400MB)
   - git-large-coco (~800MB)
4. **TrOCR** (Microsoft) - for text in images
   - trocr-base-printed
   - trocr-small-handwritten

### Hardware Acceleration Options
```python
# CPU optimization
providers = ['CPUExecutionProvider']

# NVIDIA GPU support  
providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']

# DirectML for Windows NPU/GPU
providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
```

## 3. Detailed Image Description Models

### Problem
Current models focus on short captions rather than comprehensive descriptions needed for accessibility and detailed analysis.

### LLaVA (Large Language and Vision Assistant)
- **ONNX Available**: ‚úÖ Multiple versions found
  - Esperanto/llava-1.5-7b-kvc-fp16-onnx (~13GB)
  - Esperanto/llava-1.6-kvc-fp16-onnx  
  - Esperanto/llava-1.5-7b-kvc-AWQ-int4-onnx (~3.5GB quantized)
- **Capabilities**:
  - Responds to natural language prompts
  - Generates detailed descriptions when prompted
  - Accessibility-focused when prompted appropriately
  - Can follow specific instruction formats

### BLIP-2 and InstructBLIP
- **Capabilities**: Advanced vision-language models for detailed descriptions
- **ONNX Status**: Would need custom conversion
- **Features**: Instruction-following for specific description styles

### Model Comparison
| Model | Description Quality | ONNX Available | Size | Accessibility Focus |
|-------|-------------------|----------------|------|-------------------|
| **LLaVA-1.5-7B** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ Yes | ~7B | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **BLIP-2** | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è Custom | ~3B | ‚≠ê‚≠ê‚≠ê |
| **InstructBLIP** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è Custom | ~7B | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Windows AI API** | ‚≠ê‚≠ê‚≠ê‚≠ê | üîí Built-in | Unknown | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

## 4. Implementation Strategy

### Phase 1: ONNX Runtime Provider
```python
class ONNXProvider(AIProvider):
    def get_available_models(self) -> List[str]:
        return [
            "blip-image-captioning-base.onnx",
            "blip-image-captioning-large.onnx", 
            "vit-gpt2-image-captioning.onnx",
            "llava-1.5-7b-detailed.onnx"
        ]
    
    def describe_image(self, image_path: str, prompt: str, model: str) -> str:
        # Load model if not already loaded
        if self.session is None:
            self._load_model(model)
        
        # Preprocess image and run inference
        image_tensor = self._preprocess_image(image_path)
        outputs = self.session.run(None, {"image": image_tensor})
        return self._postprocess_output(outputs[0])
```

### Phase 2: Copilot+ PC Provider  
```python
class CopilotProvider(AIProvider):
    def __init__(self):
        self.requires_copilot_pc = True
        self._check_availability()
    
    def _check_availability(self):
        # Check if running on Copilot+ PC with required NPU
        # Check Windows version and updates
        # Check if Windows AI APIs are available
        
    def get_available_models(self) -> List[str]:
        return [
            "Windows.AI.ImageDescription.Brief",
            "Windows.AI.ImageDescription.Detailed", 
            "Windows.AI.ImageDescription.Accessible",
            "Windows.AI.PhiSilica"
        ]
```

### Phase 3: Enhanced Prompting for Detailed Descriptions
```python
DETAILED_PROMPTS = {
    "accessibility": """Describe this image in detail for someone who is visually impaired. Include:
- Overall scene and setting
- People present (age, gender, clothing, expressions, activities)  
- Objects and their locations
- Colors and visual style
- Any text visible in the image
- Spatial relationships between elements
- Mood or atmosphere of the scene""",

    "comprehensive": """Provide a thorough, detailed description of this image including:
- Main subjects and background
- Visual composition and layout
- Colors, lighting, and artistic style
- All text content if present
- Emotional context or story being told
- Technical details if relevant""",

    "structured": """Analyze this image systematically:
1. Setting/Location: [describe]
2. People: [describe each person]
3. Objects: [list and describe major objects]
4. Text: [transcribe any visible text]
5. Colors/Style: [describe visual appearance]
6. Context: [explain what's happening]"""
}
```

## 5. What Windows Narrator Likely Uses

Based on research, Windows Narrator likely uses:
- Microsoft's custom-trained models for accessibility
- Windows AI Image Description API with 'Accessible' mode
- Possibly BLIP-family models fine-tuned for detailed descriptions
- Microsoft's Seeing AI technology (from mobile app)

## 6. Recommended Implementation Priority

1. **ONNX Runtime Support** (High Priority)
   - Immediate performance benefits
   - Cross-platform compatibility
   - Large ecosystem of models

2. **LLaVA Integration** (High Priority)  
   - Best detailed descriptions available
   - ONNX versions ready to use
   - Excellent for accessibility

3. **Enhanced Prompting** (Medium Priority)
   - Improve existing providers
   - Better descriptions with current models

4. **Copilot+ PC Support** (Medium Priority)
   - Future-proofing for new hardware
   - Native Windows integration

## 7. Provider Comparison

| Feature | Ollama | OpenAI | HuggingFace | **ONNX** | **Copilot+** |
|---------|--------|--------|-------------|----------|-------------|
| **Performance** | Good | Fast (cloud) | Variable | **Excellent** | **Excellent** |
| **Offline** | ‚úÖ | ‚ùå | ‚úÖ | **‚úÖ** | **‚úÖ** |
| **Cost** | Free | Paid | Free | **Free** | **Free** |
| **Setup** | Complex | Easy | Medium | **Easy** | **Medium** |
| **Hardware Acceleration** | Limited | N/A | Basic | **Full** | **Native** |
| **Cross-platform** | Good | ‚úÖ | Good | **Excellent** | ‚ùå |

## 8. Research References

- Windows AI Foundry documentation: https://learn.microsoft.com/en-us/windows/ai/
- ONNX Model Zoo: https://github.com/onnx/models
- LLaVA research: https://llava-vl.github.io/
- BLIP-2 paper: https://arxiv.org/abs/2301.12597
- Microsoft Seeing AI: https://www.microsoft.com/en-us/ai/seeing-ai
- Hugging Face ONNX models: https://huggingface.co/models?library=onnx

## 9. Ollama Cloud Provider Implementation ‚úÖ

**IMPLEMENTED**: ImageDescriber now includes a dedicated "Ollama Cloud" provider!

### Features
- **Separate Provider**: Appears as "Ollama Cloud" in the provider dropdown
- **Automatic Detection**: Only shows when cloud models are available (user signed in)
- **Cloud Model Filtering**: Shows only models ending in `-cloud`
- **Same API**: Uses identical Ollama API endpoints, just with cloud models
- **Easy Setup**: Users just need `ollama signin` and `ollama pull model-name:cloud`

### Usage Instructions
1. **Sign in to Ollama Cloud**:
   ```bash
   ollama signin
   ```

2. **Pull desired cloud models**:
   ```bash
   ollama pull deepseek-v3.1:671b-cloud
   ollama pull qwen3-coder:480b-cloud
   ollama pull gpt-oss:120b-cloud
   ```

3. **Use in ImageDescriber**:
   - Select "Ollama Cloud" from provider dropdown
   - Choose from available cloud models
   - Generate descriptions with massive models!

### Benefits Over Regular Ollama
- **671B parameter models** (vs typical 7-13B local models)
- **Datacenter performance** (faster than most local setups)
- **No VRAM requirements** (runs on any computer)
- **Same familiar interface** (no learning curve)

### Model Comparison Update
| Model | Parameters | Provider | Requirements | Description Quality |
|-------|------------|----------|--------------|-------------------|
| llama3.2-vision | 11B | Ollama (Local) | 8GB+ VRAM | ‚≠ê‚≠ê‚≠ê |
| **deepseek-v3.1-cloud** | **671B** | **Ollama Cloud** | **Internet only** | **‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê** |
| **qwen3-coder-cloud** | **480B** | **Ollama Cloud** | **Internet only** | **‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê** |
| gpt-4o | Unknown | OpenAI | API key + $ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

## 10. Next Steps

1. **Research Phase Complete** ‚úÖ
2. **Ollama Cloud Provider** ‚úÖ **IMPLEMENTED**
3. Implement basic ONNX Runtime provider
4. Test LLaVA models for description quality  
5. Research Windows AI API Python bindings
6. Design hardware detection and fallback logic
7. Create comprehensive testing strategy
8. Performance benchmarking across providers

This enhancement would position ImageDescriber as a cutting-edge tool capable of leveraging the latest AI hardware while providing high-quality, accessibility-focused image descriptions.