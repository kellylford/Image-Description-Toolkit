#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Analyze description content and word frequencies across AI models.

This script analyzes the descriptions generated by different AI models,
comparing vocabulary, word frequencies, and description characteristics.
"""

import argparse
import re
import csv
import sys
from pathlib import Path
from collections import Counter, defaultdict

# Add parent directory to path for resource manager import
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
if str(parent_dir) not in sys.path:
    sys.path.insert(0, str(parent_dir))

try:
    from scripts.resource_manager import get_resource_path
except ImportError:
    # Fallback if resource manager not available
    def get_resource_path(relative_path):
        return Path(__file__).parent.parent / relative_path
from typing import Dict, List, Tuple
import statistics
from analysis_utils import get_safe_filename, ensure_directory

# Fix Windows terminal encoding issues
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')


# Common stopwords to filter out (basic English stopwords)
STOPWORDS = {
    'a', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by',
    'for', 'from', 'has', 'have', 'he', 'her', 'his', 'i', 'in', 'is',
    'it', 'its', 'of', 'on', 'or', 'that', 'the', 'this', 'to', 'was',
    'were', 'will', 'with', 'you', 's', 't', 'd', 'm', 've', 'll', 're'
}

# Descriptive word categories
COLOR_WORDS = {
    'red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown',
    'black', 'white', 'gray', 'grey', 'golden', 'silver', 'bronze', 'beige',
    'tan', 'cream', 'navy', 'maroon', 'teal', 'turquoise', 'violet', 'indigo',
    'crimson', 'scarlet', 'emerald', 'azure', 'multicolored', 'colorful',
    'vibrant', 'bright', 'dark', 'light', 'pale', 'deep'
}

EMOTION_WORDS = {
    'happy', 'sad', 'joyful', 'excited', 'peaceful', 'calm', 'serene',
    'cheerful', 'melancholic', 'dramatic', 'energetic', 'relaxed', 'tense',
    'playful', 'serious', 'somber', 'lively', 'warm', 'cold', 'inviting',
    'mysterious', 'intimate', 'grand', 'cozy', 'elegant', 'rustic'
}

TECHNICAL_WORDS = {
    'composition', 'perspective', 'focal', 'depth', 'contrast', 'lighting',
    'exposure', 'saturation', 'hue', 'tone', 'texture', 'resolution',
    'framing', 'angle', 'symmetry', 'balance', 'foreground', 'background',
    'bokeh', 'blur', 'focus', 'sharp', 'crisp', 'grain', 'noise'
}


def print_section_header(title: str):
    """Print a formatted section header."""
    print("\n" + "=" * 80)
    print(f"{title:^80}")
    print("=" * 80)


def clean_text(text: str) -> str:
    """Clean and normalize text for analysis."""
    if not text:
        return ""
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation but keep spaces
    text = re.sub(r'[^\w\s]', ' ', text)
    # Remove extra whitespace
    text = ' '.join(text.split())
    return text


def extract_words(text: str, remove_stopwords: bool = True) -> List[str]:
    """Extract words from text."""
    cleaned = clean_text(text)
    words = cleaned.split()
    
    if remove_stopwords:
        words = [w for w in words if w not in STOPWORDS and len(w) > 2]
    
    return words


def analyze_description(description: str) -> Dict:
    """Analyze a single description."""
    words_with_stopwords = extract_words(description, remove_stopwords=False)
    words = extract_words(description, remove_stopwords=True)
    
    # Count sentences (rough estimate)
    sentences = len(re.split(r'[.!?]+', description))
    
    # Categorize words
    colors = sum(1 for w in words if w in COLOR_WORDS)
    emotions = sum(1 for w in words if w in EMOTION_WORDS)
    technical = sum(1 for w in words if w in TECHNICAL_WORDS)
    
    return {
        'total_words': len(words_with_stopwords),
        'unique_words': len(set(words)),
        'content_words': len(words),  # Without stopwords
        'sentences': max(1, sentences - 1),  # Adjust for split behavior
        'color_words': colors,
        'emotion_words': emotions,
        'technical_words': technical,
        'words': words
    }


def analyze_model_descriptions(model_name: str, descriptions: List[str]) -> Dict:
    """Analyze all descriptions for a model."""
    all_stats = []
    all_words = []
    word_counter = Counter()
    
    for desc in descriptions:
        if desc and desc.strip():
            stats = analyze_description(desc)
            all_stats.append(stats)
            all_words.extend(stats['words'])
            word_counter.update(stats['words'])
    
    if not all_stats:
        return None
    
    # Calculate aggregate statistics
    total_words_list = [s['total_words'] for s in all_stats]
    unique_words_list = [s['unique_words'] for s in all_stats]
    sentence_counts = [s['sentences'] for s in all_stats]
    
    return {
        'model': model_name,
        'description_count': len(all_stats),
        'total_words': sum(total_words_list),
        'avg_words_per_desc': statistics.mean(total_words_list) if total_words_list else 0,
        'avg_unique_per_desc': statistics.mean(unique_words_list) if unique_words_list else 0,
        'avg_sentences': statistics.mean(sentence_counts) if sentence_counts else 0,
        'vocabulary_size': len(set(all_words)),
        'vocabulary_richness': len(set(all_words)) / len(all_words) * 100 if all_words else 0,
        'word_frequencies': word_counter,
        'color_mentions': sum(s['color_words'] for s in all_stats),
        'emotion_mentions': sum(s['emotion_words'] for s in all_stats),
        'technical_mentions': sum(s['technical_words'] for s in all_stats),
    }


def detect_delimiter(csv_path: Path) -> str:
    """
    Detect the delimiter used in a CSV file by reading the first line.
    
    Returns:
        The detected delimiter character (',', '\\t', or '@')
    """
    with open(csv_path, 'r', encoding='utf-8') as f:
        first_line = f.readline()
        
        # Count occurrences of each potential delimiter
        comma_count = first_line.count(',')
        tab_count = first_line.count('\t')
        at_count = first_line.count('@')
        
        # Return the delimiter with the highest count
        if tab_count > comma_count and tab_count > at_count:
            return '\t'
        elif comma_count > at_count:
            return ','
        else:
            return '@'


def read_combined_descriptions(csv_path: Path) -> Dict[str, List[str]]:
    """Read the combined descriptions CSV file with auto-detected delimiter."""
    # Auto-detect the delimiter
    delimiter = detect_delimiter(csv_path)
    delimiter_name = 'comma' if delimiter == ',' else 'tab' if delimiter == '\t' else '@'
    print(f"Detected delimiter: {delimiter_name}")
    
    model_descriptions = defaultdict(list)
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter=delimiter)
        
        for row in reader:
            # Skip the filename column
            for model_name, description in row.items():
                if model_name and description and model_name.lower() != 'image name':
                    model_descriptions[model_name].append(description)
    
    return dict(model_descriptions)


def print_model_stats(stats: Dict):
    """Print statistics for a single model."""
    print(f"\n{stats['model']}")
    print("-" * 80)
    print(f"  Descriptions Analyzed:   {stats['description_count']:,}")
    print(f"  Total Words:             {stats['total_words']:,}")
    print(f"  Vocabulary Size:         {stats['vocabulary_size']:,} unique words")
    print(f"  Vocabulary Richness:     {stats['vocabulary_richness']:.1f}%")
    print(f"\n  Per Description:")
    print(f"    Avg Words:             {stats['avg_words_per_desc']:.1f}")
    print(f"    Avg Unique Words:      {stats['avg_unique_per_desc']:.1f}")
    print(f"    Avg Sentences:         {stats['avg_sentences']:.1f}")
    
    if stats['color_mentions'] or stats['emotion_mentions'] or stats['technical_mentions']:
        print(f"\n  Descriptive Categories:")
        print(f"    Color Words:           {stats['color_mentions']} times")
        print(f"    Emotion/Tone Words:    {stats['emotion_mentions']} times")
        print(f"    Technical Terms:       {stats['technical_mentions']} times")
    
    print(f"\n  Top 15 Words:")
    for i, (word, count) in enumerate(stats['word_frequencies'].most_common(15), 1):
        print(f"    {i:2}. {word:<20} ({count:3} times)")


def find_common_and_unique_words(all_stats: List[Dict]) -> Dict:
    """Find words common to all models and unique to each model."""
    if not all_stats:
        return {}
    
    # Get word sets for each model
    model_words = {s['model']: set(s['word_frequencies'].keys()) for s in all_stats}
    
    # Find words common to all models
    common_words = set.intersection(*model_words.values()) if model_words else set()
    
    # Find unique words for each model
    unique_words = {}
    for model, words in model_words.items():
        other_words = set()
        for other_model, other_model_words in model_words.items():
            if other_model != model:
                other_words.update(other_model_words)
        unique_words[model] = words - other_words
    
    return {
        'common_words': common_words,
        'unique_words': unique_words,
        'model_words': model_words
    }


def print_comparison(all_stats: List[Dict]):
    """Print comparison across all models."""
    print_section_header("CROSS-MODEL COMPARISON")
    
    # Sort by vocabulary richness
    sorted_by_richness = sorted(all_stats, key=lambda x: x['vocabulary_richness'], reverse=True)
    
    print("\n📚 Vocabulary Richness (% unique words):")
    for i, stats in enumerate(sorted_by_richness, 1):
        print(f"  {i}. {stats['model']:<35} {stats['vocabulary_richness']:5.1f}% ({stats['vocabulary_size']:,} unique words)")
    
    # Sort by verbosity
    sorted_by_words = sorted(all_stats, key=lambda x: x['avg_words_per_desc'], reverse=True)
    
    print("\n📝 Average Description Length (words/description):")
    for i, stats in enumerate(sorted_by_words, 1):
        print(f"  {i}. {stats['model']:<35} {stats['avg_words_per_desc']:6.1f} words/description")
    
    # Sort by color usage
    sorted_by_colors = sorted(all_stats, key=lambda x: x['color_mentions'], reverse=True)
    
    print("\n🎨 Color Word Usage:")
    for i, stats in enumerate(sorted_by_colors, 1):
        per_desc = stats['color_mentions'] / stats['description_count'] if stats['description_count'] else 0
        print(f"  {i}. {stats['model']:<35} {stats['color_mentions']:3} times ({per_desc:.1f} per description)")
    
    # Word overlap analysis
    word_analysis = find_common_and_unique_words(all_stats)
    
    print("\n🔤 Word Overlap Analysis:")
    print(f"  Common words (used by ALL models): {len(word_analysis['common_words'])} words")
    if word_analysis['common_words']:
        common_sample = list(word_analysis['common_words'])[:20]
        print(f"    Sample: {', '.join(sorted(common_sample))}")
    
    print("\n  Unique words (used by only ONE model):")
    for model, unique in word_analysis['unique_words'].items():
        if unique:
            unique_sample = list(unique)[:10]
            print(f"    {model}: {len(unique)} unique words")
            print(f"      Sample: {', '.join(sorted(unique_sample))}")


def save_analysis_csv(all_stats: List[Dict], output_file: Path):
    """Save analysis to CSV file."""
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # Header
        writer.writerow([
            'Model',
            'Descriptions',
            'Total Words',
            'Vocabulary Size',
            'Vocabulary Richness %',
            'Avg Words/Description',
            'Avg Unique Words/Description',
            'Avg Sentences',
            'Color Words',
            'Emotion Words',
            'Technical Words',
            'Top 5 Words'
        ])
        
        # Data rows
        for stats in all_stats:
            top_5 = ', '.join([f"{w}({c})" for w, c in stats['word_frequencies'].most_common(5)])
            
            writer.writerow([
                stats['model'],
                stats['description_count'],
                stats['total_words'],
                stats['vocabulary_size'],
                f"{stats['vocabulary_richness']:.1f}",
                f"{stats['avg_words_per_desc']:.1f}",
                f"{stats['avg_unique_per_desc']:.1f}",
                f"{stats['avg_sentences']:.1f}",
                stats['color_mentions'],
                stats['emotion_mentions'],
                stats['technical_mentions'],
                top_5
            ])
    
    print(f"\nAnalysis saved to: {output_file}")


def main():
    """Main function."""
    parser = argparse.ArgumentParser(
        description='Analyze word frequencies and vocabulary across AI model descriptions.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
Examples:
  # Analyze default combined descriptions file (auto-detects CSV/TSV/ATSV format)
  python analyze_description_content.py
  
  # Analyze specific combined descriptions file
  python analyze_description_content.py --input my_descriptions.csv
  
  # Analyze TSV file
  python analyze_description_content.py --input results.tsv
  
  # Specify custom output filename
  python analyze_description_content.py --output word_analysis.csv
  
  # Full custom usage
  python analyze_description_content.py --input my_descriptions.csv --output my_analysis.csv
        '''
    )
    
    parser.add_argument(
        '--input',
        type=str,
        default='combineddescriptions.csv',
        help='Input file with combined descriptions (CSV/TSV/ATSV - auto-detected, default: combineddescriptions.csv from results/ directory)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='description_content_analysis.csv',
        help='Output CSV filename (default: description_content_analysis.csv). Saved in analysis/results/ directory.'
    )
    
    args = parser.parse_args()
    
    # Find the combined descriptions file - check results/ directory first, then analysis/ directory
    # Default to results directory in current working directory
    csv_path = Path.cwd() / "results" / args.input
    
    if not csv_path.exists():
        # Fall back to analysis directory for backward compatibility
        csv_path = get_resource_path(f"analysis/{args.input}")
    
    if not csv_path.exists():
        print(f"Error: Combined descriptions file not found at: {csv_path}")
        print("\nPlease run combine_workflow_descriptions.py first to generate the file,")
        print("or specify the correct file with --input")
        return 1
    
    print_section_header("DESCRIPTION CONTENT ANALYSIS")
    print(f"Reading descriptions from: {csv_path.name}\n")
    
    # Read descriptions
    model_descriptions = read_combined_descriptions(csv_path)
    
    if not model_descriptions:
        print("No descriptions found in the file!")
        return
    
    print(f"Found {len(model_descriptions)} models with descriptions\n")
    
    # Analyze each model
    all_stats = []
    
    print_section_header("INDIVIDUAL MODEL ANALYSIS")
    
    for model_name in sorted(model_descriptions.keys()):
        descriptions = model_descriptions[model_name]
        stats = analyze_model_descriptions(model_name, descriptions)
        
        if stats:
            all_stats.append(stats)
            print_model_stats(stats)
    
    # Print comparison
    if len(all_stats) > 1:
        print_comparison(all_stats)
    
    # Save to CSV in analysis/results directory with safe filename
    output_dir = get_resource_path("analysis/results")
    ensure_directory(output_dir)
    
    output_csv = output_dir / args.output
    output_csv = get_safe_filename(output_csv)
    
    save_analysis_csv(all_stats, output_csv)
    
    print("\n" + "=" * 80)
    print("Analysis complete!")
    print(f"Results saved to: {output_csv}")
    print("=" * 80)


if __name__ == "__main__":
    main()
